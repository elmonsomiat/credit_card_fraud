{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from rainforest_functions import vectorize_categories\n",
    "from rainforest_functions import softmax_mine\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\\\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypath = '/Users/AnaSolaguren-Beascoa/Pictures/train-jpg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#in this part we read the train data before adding the rotated images\n",
    "#########################################################\n",
    "\n",
    "#read the image names and their categories\n",
    "image_name_df = pd.read_csv('train_v2.csv')\n",
    "categories = image_name_df['tags'].str.split(' ', expand=True).stack().unique()\n",
    "\n",
    "#read the green pixels for each image          \n",
    "pixel_df = pd.read_csv(mypath+'pixels_G.csv')\n",
    "pixel_df = pixel_df.rename(columns = {str(pixel_df.shape[1]-1):'image_name'})\n",
    "\n",
    "#merge the categories and the pixels\n",
    "result = pd.merge(pixel_df, image_name_df, on='image_name')\n",
    "del(pixel_df)\n",
    "\n",
    "#THESE ARE THE PIXELS FOR THE TRAIN DATA\n",
    "X_G = result.drop(['tags','image_name'],axis=1)\n",
    "#THESE ARE THE CATEGORIES FOR THE TRAIN DATA\n",
    "y_onefile = result['tags']\n",
    "del(result)\n",
    "\n",
    "#vectorise the categories\n",
    "y_G = vectorize_categories(categories, y_onefile)\n",
    "y_G.columns = categories    \n",
    "del(y_onefile)\n",
    "\n",
    "\n",
    "print('END')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#in this part we read the test data\n",
    "#########################################################\n",
    "\n",
    "#read the test pixels\n",
    "pixel_df_test = pd.read_csv(mypath+'pixels_test.csv')\n",
    "pixel_df_test = pixel_df_test.rename(columns = {str(pixel_df_test.shape[1]-1):'image_name'})\n",
    "\n",
    "#THESE ARE THE PIXELS FOR THE TEST DATA\n",
    "X_G_test = pixel_df_test.drop(['image_name'],axis=1)\n",
    "X_G_test = pd.DataFrame(scaler.fit_transform(X_G_test))\n",
    "\n",
    "#THESE ARE THE NAMES OF THE FILES FOR THE TEST DATA\n",
    "test_image_name = pixel_df_test['image_name']\n",
    "del(pixel_df_test)\n",
    "  \n",
    "\n",
    "print('END')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_pixels_agriculture_G.csvis read\n",
      "category is agriculture\n",
      "Now the NN is being trained\n",
      "step 0, training accuracy 0.58\n",
      "step 10, training accuracy 0.56\n",
      "step 20, training accuracy 0.52\n",
      "step 30, training accuracy 0.63\n",
      "step 40, training accuracy 0.65\n",
      "step 50, training accuracy 0.63\n",
      "step 60, training accuracy 0.62\n",
      "step 70, training accuracy 0.69\n",
      "step 80, training accuracy 0.67\n",
      "step 90, training accuracy 0.58\n",
      "step 100, training accuracy 0.7\n",
      "step 110, training accuracy 0.71\n",
      "step 120, training accuracy 0.64\n",
      "step 130, training accuracy 0.75\n",
      "step 140, training accuracy 0.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8ca5f86d145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_G_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_G_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0my_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 run_metadata):\n\u001b[1;32m   1116\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32mC:\\Users\\AnaSolaguren-Beascoa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1166\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#######################################################################################\n",
    "#in this part we read the rotated files which symmetrise our data\n",
    "#######################################################################################\n",
    "rotated_files = [f for f in listdir(mypath+'all_rotated/') if isfile(join(mypath+'all_rotated/', f))]\n",
    "num_files = len(rotated_files)\n",
    "\n",
    "######################################################################################################\n",
    "#This will be our final DataFrame with the corresponding categories for the test data\n",
    "######################################################################################################\n",
    "y_final_df = pd.DataFrame(columns=list(categories))\n",
    "\n",
    "for filenr in range(len(rotated_files)):\n",
    "\n",
    "    ########################################################################\n",
    "    #read the rotated pictures for green color \n",
    "    ########################################################################\n",
    "      \n",
    "    if rotated_files[filenr].endswith('G.csv'):\n",
    "        \n",
    "        \n",
    "        pixel_df= pd.read_csv(mypath+'all_rotated/'+rotated_files[filenr])\n",
    "        X_onefile = pixel_df.drop(categories.tolist(),axis=1)\n",
    "        y_vectorised_one = pixel_df[categories.tolist()]\n",
    "        del(pixel_df)\n",
    "    \n",
    "        X_G_2 = pd.concat([X_G, X_onefile], ignore_index=True)\n",
    "        y_G_2 = pd.concat([y_G, y_vectorised_one], ignore_index=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(rotated_files[filenr] + 'is read')\n",
    "        \n",
    "        #read the category of the file\n",
    "        for cat_type in categories:\n",
    "            if cat_type in rotated_files[filenr]:\n",
    "                current_category = cat_type\n",
    "                break\n",
    "        print('category is ' + current_category)       \n",
    "        \n",
    "        X_data = X_G_2.copy()\n",
    "        y_pre = y_G_2[cat_type]\n",
    "        del(X_G_2)\n",
    "        del(y_G_2)\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        #create two columns for the output, true contains the true value\n",
    "        ########################################################################\n",
    "\n",
    "        \n",
    "        y_data = pd.DataFrame(columns=['True','False'], index=range(y_pre.shape[0]))\n",
    "        y_data['True'] = y_pre.astype(int)\n",
    "        y_data.loc[y_data['True'] == 1, 'False'] = 0\n",
    "        y_data.loc[y_data['True'] == 0, 'False'] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###########################################################################\n",
    "            #Now, lets divide our data into test and training\n",
    "            #this step is done for each different category\n",
    "            #This step is just needed when testing how good our model is\n",
    "            #can be set to test_size=0 when calculating the final result\n",
    "        ###########################################################################\n",
    "        #split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0)\n",
    "\n",
    "        #scale the data\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "        X_test = pd.DataFrame(scaler.fit_transform(X_test))\n",
    "\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        #make a list of all the categories by order of appearence\n",
    "        category_sess = []\n",
    "        category_sess.append(cat_type)\n",
    "        \n",
    "        \n",
    "\n",
    "        #########################################################       \n",
    "        #IN THIS PART WE DEFINE OUR NEURAL NETWORK     \n",
    "        #########################################################\n",
    "        lenx = X_data.shape[1]\n",
    "        leny = y_data.shape[1]\n",
    "\n",
    "        x = tf.placeholder(tf.float32, shape=[None, lenx])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, leny])\n",
    "\n",
    "        x_image = tf.reshape(x, [-1, int(np.sqrt(lenx)), int(np.sqrt(lenx)), 1])\n",
    "        x_image = tf.cast(x_image, tf.float32)\n",
    "        #layer 1\n",
    "\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "        #layer 2\n",
    "        W_conv2 = weight_variable([20, 20, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        #layer 3\n",
    "        W_fc1 = weight_variable([int(np.sqrt(lenx)/4)*int(np.sqrt(lenx)/4)*64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, int(np.sqrt(lenx)/4)*int(np.sqrt(lenx)/4)*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        #layer 4\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "        #layer 5\n",
    "        W_fc2 = weight_variable([1024, leny])\n",
    "        b_fc2 = bias_variable([leny])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "        #output\n",
    "        y = y_conv\n",
    "\n",
    "\n",
    "        \n",
    "        #########################################################       \n",
    "        #IN THIS PART WE TRAIN OUR NEURAL NETWORK     \n",
    "        #########################################################\n",
    "        #train the network\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "        learning_rate = 0.000003\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        batchsize = 100\n",
    "        steps = int(X_train.shape[0]/batchsize)\n",
    "     \n",
    "        \n",
    "        #make a list which contains each session\n",
    "        sess=tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        print('Now the NN is being trained')\n",
    "        \n",
    "        i = 0\n",
    "        for k in range(steps):\n",
    "            batch_xs = X_train.iloc[i:i+batchsize,:].as_matrix().astype(np.float32)\n",
    "            batch_ys = y_train.iloc[i:i+batchsize].as_matrix().astype(np.float32)\n",
    "            if k % 10 == 0:\n",
    "                train_accuracy = sess.run(accuracy,feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "                print('step %d, training accuracy %g' % (k, train_accuracy))\n",
    "                learning_rate = learning_rate*0.85\n",
    "                \n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "            \n",
    "            i+=batchsize\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        #########################################################       \n",
    "        #IN THIS PART CALCULATE THE RESULT FOR THE TEST DATA\n",
    "        #########################################################\n",
    "        \n",
    "        y_final=[]\n",
    "        \n",
    "        for p in range(len(X_G_test)-1):\n",
    "            answer = sess.run(y_conv, feed_dict={x: X_G_test[p:p+1], keep_prob: 1})\n",
    "            y_pred_test = sess.run(tf.nn.softmax(answer,dim=-1,name=None)).astype(int)\n",
    "            y_final.append(y_pred_test[0][0].astype(int))\n",
    "        \n",
    "            \n",
    "        \n",
    "        y_final_df[cat_type] = y_final\n",
    "        \n",
    "   \n",
    "\n",
    "        #next file\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
